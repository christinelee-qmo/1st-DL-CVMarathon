{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python網路期末專題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、專題摘要\n",
    "### 1. 期末專題主題 :  PTT八卦板文章\n",
    "https://www.ptt.cc/bbs/Gossiping/index.html\n",
    "\n",
    "### 2. 期末專題基本目標 : \n",
    "- 從PTT政黑板爬取文章，並透過jieba將文章拆解\n",
    "- 可以簡單的計算同樣文字出現的頻率或是透過TFIDF的統計方式計算\n",
    "- 過濾stop words，對經常出現的關鍵字做排名\n",
    "- 將結果以文字雲方式呈現\n",
    "\n",
    "### 3. 期末專題進階目標 : \n",
    "- 從爬取的內容分析來自相同IP不同帳號的文章，列出有網軍嫌疑的帳號\n",
    "- 爬取帳號發過的所有文章，分析詞類分布\n",
    "- 分析帳號在特定期間的活動情形"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、實作方法介紹(介紹使用的程式碼、模組,並附上實作過程與結果的識國,需圆文並茂)\n",
    "1. 使用的程式碼介紹\n",
    "2. 使用的模組介紹\n",
    "\n",
    "## 三、成果展示(介紹成果的特點為何,並撰寫心得)\n",
    "## 四、結論(總結本次專題的問題與結果)\n",
    "## 五、期末專題作者資訊(請附上作者資訊)\n",
    "1. 個人Github連結\n",
    "2. 個人在百日馬拉松顯示名稱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、實作方法介紹(介紹使用的程式碼、模組,並附上實作過程與結果的識國,需圆文並茂)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (一)爬取資料\n",
    "使用requests和BeautifulSoup對PTT政黑版進行爬蟲，爬取前50頁所有文章(約1000篇)。\n",
    "我使用的方法是先將所有文章的網址爬出來後歸納至list，再由list內的網址去爬取其內文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PTT_URL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-eb1498ee8aff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;31m# 新爬PTT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m \u001b[0mcrawlTest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawlPTT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPTT_URL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0mcrawlTest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrawl_commend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 爬500篇文章及其推文\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0mcrawlTest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveCrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PTT_URL' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import _thread\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PTT_URL = 'https://www.ptt.cc/bbs/HatePolitics/index.html'\n",
    "\n",
    "class CrawlPTT:\n",
    "    def __init__(self, PTT_URL):\n",
    "        self.PTT_URL = PTT_URL\n",
    "        self.data = []       # 存全部發文的資訊(發文者、內容、ip)\n",
    "        self.twit = []       # 存全部推文的資訊(發文者、內容、ip)\n",
    "        self.allArticle = '' # 存全部發文的內容\n",
    "    \n",
    "    def crawl_article(self, url): # 從D025作業示範的程式碼稍作修改而來\n",
    "        response = requests.get(url, cookies={'over18': '1'})        \n",
    "        ## 假設網頁回應不是 200 OK 的話, 我們視為傳送請求失敗\n",
    "        if response.status_code != 200:\n",
    "            print('Error - {} is not available to access'.format(url))\n",
    "            return        \n",
    "        ## 將網頁回應的 HTML 傳入 BeautifulSoup 解析器, 方便我們根據標籤 (tag) 資訊去過濾尋找\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")        \n",
    "        ## 取得文章內容主體\n",
    "        main_content = soup.find(id='main-content')        \n",
    "        ## 假如文章有屬性資料 (meta), 我們在從屬性的區塊中爬出作者 (author), 文章標題 (title), 發文日期 (date)\n",
    "        metas = main_content.select('div.article-metaline')\n",
    "        author = ''\n",
    "        title = ''\n",
    "        if metas:\n",
    "            if metas[0].select('span.article-meta-value')[0]:\n",
    "                author = metas[0].select('span.article-meta-value')[0].string\n",
    "            if metas[1].select('span.article-meta-value')[0]:\n",
    "                title = metas[1].select('span.article-meta-value')[0].string\n",
    "            for m in metas:\n",
    "                m.extract()\n",
    "            for m in main_content.select('div.article-metaline-right'):\n",
    "                m.extract()        \n",
    "        ## 取得留言區主體\n",
    "        pushes = main_content.find_all('div', class_='push')\n",
    "        for p in pushes:\n",
    "            p.extract()        \n",
    "        ## 假如文章中有包含「※ 發信站: 批踢踢實業坊(ptt.cc), 來自: xxx.xxx.xxx.xxx」的樣式\n",
    "        ## 透過 regular expression 取得 IP\n",
    "        ## 因為字串中包含特殊符號跟中文, 這邊建議使用 unicode 的型式 u'...'\n",
    "        try:\n",
    "            ip = main_content.find(text=re.compile(u'※ 發信站:'))\n",
    "            ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n",
    "        except Exception as e:\n",
    "            ip = ''        \n",
    "        ## 移除文章主體中 '※ 發信站:', '◆ From:', 空行及多餘空白 (※ = u'\\u203b', ◆ = u'\\u25c6')\n",
    "        ## 保留英數字, 中文及中文標點, 網址, 部分特殊符號        \n",
    "        filtered = []\n",
    "        for v in main_content.stripped_strings:\n",
    "            ## 假如字串開頭不是特殊符號或是以 '--' 開頭的, 我們都保留其文字\n",
    "            if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--']:\n",
    "                filtered.append(v)    \n",
    "        ## 定義一些特殊符號與全形符號的過濾器\n",
    "        expr = re.compile(u'[^一-龥。；，：“”（）、？《》\\s\\w:/-_.?~%()]')\n",
    "        for i in range(len(filtered)):\n",
    "            filtered[i] = re.sub(expr, '', filtered[i])        \n",
    "        ## 移除空白字串, 組合過濾後的文字即為文章本文 (content)\n",
    "        filtered = [i for i in filtered if i]\n",
    "        content = ' '.join(filtered)\n",
    "        content = content.replace('\\n', '')\n",
    "        urlList = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', content) \n",
    "        for url in urlList:\n",
    "            content = content.replace(url, \"\")        \n",
    "        messages = []\n",
    "        for push in pushes:\n",
    "            ## 假如留言段落沒有 push-tag 就跳過\n",
    "            if not push.find('span', 'push-tag'):\n",
    "                continue            \n",
    "            ## 過濾額外空白與換行符號\n",
    "            ## push_tag 判斷是推文, 箭頭還是噓文\n",
    "            ## push_userid 判斷留言的人是誰\n",
    "            ## push_content 判斷留言內容\n",
    "            ## push_ipdatetime 判斷留言日期時間\n",
    "            push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n",
    "            if push_tag == u'推':\n",
    "                push_tag = 'p'\n",
    "            elif push_tag == u'噓':\n",
    "                push_tag = 'b'\n",
    "            else:\n",
    "                push_tag = 'n'\n",
    "            push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n",
    "            if not push_userid ==[]:\n",
    "                push_content = push.find('span', 'push-content').strings\n",
    "                push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')\n",
    "                push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r').split(' ')[0]\n",
    "\n",
    "                ## 整理打包留言的資訊\n",
    "                self.twit.append({\n",
    "                    'push_tag': push_tag,\n",
    "                    'push_userid': push_userid,\n",
    "                    'push_content': push_content,\n",
    "                    'push_ipdatetime': push_ipdatetime})\n",
    "        \n",
    "        ## 整理文章資訊\n",
    "        data = {\n",
    "            'article_author': author.split(' ')[0],\n",
    "            'article_title': title,\n",
    "            'article_content': content,\n",
    "            'ip': ip,\n",
    "        }\n",
    "        self.data.append(data)\n",
    "    \n",
    "    def Crawl_commend(self, CrawlAmount=50, nextPage=None): # 從D025作業示範的程式碼稍作修改而來\n",
    "        if nextPage == None:\n",
    "            nextPage = self.PTT_URL\n",
    "        else:\n",
    "            nextPage = 'https://www.ptt.cc' + nextPage\n",
    "        \n",
    "        # 對文章列表送出請求並取得列表主體\n",
    "        resp = requests.get(nextPage, cookies={'over18': '1'})\n",
    "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "        nextPage = soup.find('div', 'btn-group btn-group-paging').find_all('a')[1].attrs['href'] # 抓下頁網址\n",
    "        main_list = soup.find('div', class_='bbs-screen')\n",
    "        # 依序檢查文章列表中的 tag, 遇到分隔線就結束, 忽略這之後的文章\n",
    "        for div in main_list.findChildren('div', recursive=False):\n",
    "            if len(self.data) > CrawlAmount: # 超過指定取的文章數量即結束，但因為使用thread所以數量不正確\n",
    "                return\n",
    "            class_name = div.attrs['class']\n",
    "            \n",
    "            # 遇到分隔線要處理的情況\n",
    "            if class_name and 'r-list-sep' in class_name:\n",
    "                break\n",
    "            # 遇到目標文章\n",
    "            if class_name and 'r-ent' in class_name:\n",
    "                div_title = div.find('div', class_='title')\n",
    "                a_title = div_title.find('a', href=True)\n",
    "                if a_title == None:\n",
    "                    continue\n",
    "                article_URL = urljoin(self.PTT_URL, a_title['href'])\n",
    "                article_title = a_title.text\n",
    "                \n",
    "                # 呼叫上面寫好的 function 來對文章進行爬蟲\n",
    "                # 使用thread加速\n",
    "                _thread.start_new_thread( self.crawl_article, (article_URL, ) ) \n",
    "        \n",
    "        self.Crawl_commend(CrawlAmount, nextPage)\n",
    "    \n",
    "    def saveCrawl(self):\n",
    "        # 將爬完的資訊存成 json 檔案\n",
    "        with open('parse_data.json', 'w+', encoding='utf8') as f:\n",
    "            json.dump(self.data, f, ensure_ascii=False, indent=4)\n",
    "        with open('parse_twit.json', 'w+', encoding='utf8') as f:\n",
    "            json.dump(self.twit, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "    def loadPreCrawl(self):\n",
    "        # 讀取之前存成 json 檔案的資訊\n",
    "        with open(\"parse_data.json\",'r', encoding='utf-8') as fh:\n",
    "            self.data = json.load(fh) \n",
    "        with open(\"parse_twit.json\",'r', encoding='utf-8') as fh:\n",
    "            self.twit = json.load(fh) \n",
    "            \n",
    "    \n",
    "    def collectCommend(self):\n",
    "        # 將整理好的發/推文資訊轉成dataframe再處理\n",
    "        pdtwit = pd.DataFrame(self.twit)\n",
    "        pddata = pd.DataFrame(self.data)\n",
    "        # pddata_byname：整理每個發文者全部的發文整理一起\n",
    "        pddata_byname = self.articleCollect(pddata, 'article_author', 'article_content', 'article_times')\n",
    "        # pddata_byip：整理每個ip的全部發文者\n",
    "        pddata_byip = self.articleCollect(pddata, 'ip', 'article_author', 'article_author_len')\n",
    "        # pdtwit_byname：整理每個推文者全部的發文整理一起\n",
    "        pdtwit_byname = self.articleCollect(pdtwit, 'push_userid', 'push_content', 'push_times')\n",
    "        # pdtwit_byip：整理每個ip的全部推文者\n",
    "        pdtwit_byip = self.articleCollect(pdtwit, 'push_ipdatetime', 'push_userid', 'push_userid_len')\n",
    "        return pddata_byname, pddata_byip, pdtwit_byname, pdtwit_byip\n",
    "        \n",
    "    def articleCollect(self, inputpd, author, content, times):\n",
    "        ## 整理每個推/發文id的全部發文\n",
    "        ##     每個推/發文ip的全部id\n",
    "        temp = []\n",
    "        for name in inputpd[author]: # 抓每個發/推文者的id/ip\n",
    "            if not name in temp:\n",
    "                temp.append(name)\n",
    "        tempD = pd.DataFrame(temp)   # 將每個發/推文者的id/ip預先變成dataframe以便後續填補資料\n",
    "        tempD = tempD.rename({0:author}, axis='columns')\n",
    "        temp = []       # 裝每個發/推文者的id/ip\n",
    "        tempL = []      # 裝每個id/ip的推/發文數或推/發文人數\n",
    "        allArticle = '' # 全部發文內容彙整 -> 了解最近大家關心的\n",
    "        pdCount = 0\n",
    "        for name in tempD[author]:\n",
    "            temppd = inputpd[inputpd[author]==name]\n",
    "            for detail in temppd[content]:\n",
    "                if ('author' in author)*('article' in content):\n",
    "                    allArticle += detail+' ' # 全部發文內容彙整 -> 了解最近大家關心的\n",
    "                try:\n",
    "                    if ('ip' in author):\n",
    "                        if not detail in temp[pdCount]:\n",
    "                            temp[pdCount] = temp[pdCount]+';'+detail        \n",
    "                    else:\n",
    "                        temp[pdCount] = temp[pdCount]+';'+detail        \n",
    "                except:\n",
    "                    temp.append(detail)     \n",
    "            if ('ip' in author):\n",
    "                tempL.append(len(temp[pdCount].split(';'))) # 拿到ip -> 整理此ip有幾人用\n",
    "            else:\n",
    "                tempL.append(len(temppd[content]))          # 沒拿到ip -> 整理此id發/推過幾篇文\n",
    "            pdCount +=1\n",
    "        tempD[content] = temp\n",
    "        tempD[times] = tempL\n",
    "        tempD = tempD.sort_values(by=times, ascending=False).reset_index()\n",
    "        if ('author' in author)*('article' in content):  \n",
    "            self.allArticle = allArticle\n",
    "        return tempD\n",
    "    \n",
    "    \n",
    "    \n",
    "def jiebaPTT(allArticle, topk=5, OperateF=0, countlen=10, allstopwords=None): \n",
    "    ### topk:要出現前多少名次數的詞。countlen:input為dataframe要對前幾列作jieba\n",
    "    ### allArticle接收str或DataFrame格式\n",
    "    jieba.set_dictionary('./For_jieba/dict.txt') # 使用繁體辭庫\n",
    "    jieba.load_userdict('./For_jieba/my_dict.txt')  #自定義詞彙  \n",
    "    jieba.analyse.set_stop_words('./For_jieba/stopwords.txt')\n",
    "    \n",
    "    if OperateF == 0:\n",
    "        with open( './For_jieba/stopwords.txt' ,'r', encoding = 'utf-8') as fh: # 原本stopwords\n",
    "            stopWords = fh.readlines() \n",
    "            fh.close()\n",
    "        stopWords = [ w.strip() for w in stopWords ] # strip除去 '\\n' '\\t' ' '\n",
    "        with open( './For_jieba/my_stopwords.txt' ,'r', encoding = 'utf-8') as fh: # 自訂stopwords\n",
    "            mystopWords = fh.readlines() \n",
    "            fh.close()\n",
    "        mystopWords = [ w.strip() for w in mystopWords ] \n",
    "        allstopwords = stopWords+mystopWords\n",
    "    \n",
    "    if type(allArticle) == str:   # 拿到字串 -> 對此字串做jieba\n",
    "        words = jieba.cut(allArticle, cut_all = False) #預設為False  \n",
    "        filterWords_list2 = [ w for w in words if w not in allstopwords]\n",
    "        filterWords_str = ''.join(filterWords_list2)  \n",
    "        tags = jieba.analyse.extract_tags(filterWords_str, topk)\n",
    "        if OperateF == 0:\n",
    "            temp = []\n",
    "            for t in tags:\n",
    "                temp.append(filterWords_list2.count(t))\n",
    "            tagspd = pd.DataFrame([tags, temp]).T\n",
    "            tagspd = tagspd.rename({0:'KeyWords', 1:'Times'}, axis='columns')\n",
    "            return tagspd, tags\n",
    "        else:\n",
    "            return tags, filterWords_list2\n",
    "    else:  # 拿到dataframe -> 對每個整理過的推/發文內容做jieba\n",
    "        try:\n",
    "            temp = pd.DataFrame(columns=list(range(topk+3))) # 3=(推/發文者)+(推/發文次數)+(全部關鍵字)\n",
    "            index = 0\n",
    "            try:\n",
    "                # 計算每個推文作者推文的關鍵字                \n",
    "                temp[0]=allArticle['push_userid'][0:countlen]\n",
    "                temp[1]=allArticle['push_times'][0:countlen]\n",
    "                temp = temp.fillna(0)                \n",
    "                for detail in allArticle['push_content'][0:countlen]:\n",
    "                    tags, filterWords_list2=jiebaPTT(detail, topk=topk, OperateF=1, allstopwords=allstopwords)\n",
    "                    countCOL = 2\n",
    "                    allkeys = ''\n",
    "                    for tag in tags:\n",
    "                        allkeys += tag+' '\n",
    "                        temp[countCOL][index] = tag+':'+str(filterWords_list2.count(tag))\n",
    "                        countCOL +=1\n",
    "                    temp[countCOL][index] = allkeys\n",
    "                    index +=1\n",
    "            except:\n",
    "                # 計算每個發文作者發文的關鍵字                \n",
    "                temp[0]=allArticle['article_author'][0:countlen]\n",
    "                temp[1]=allArticle['article_times'][0:countlen]\n",
    "                temp = temp.fillna(0)\n",
    "                for detail in allArticle['article_content'][0:countlen]:\n",
    "                    #print(detail)\n",
    "                    tags, filterWords_list2=jiebaPTT(detail, topk=topk, OperateF=1, allstopwords=allstopwords)\n",
    "                    countCOL = 2\n",
    "                    allkeys = ''\n",
    "                    for tag in tags:\n",
    "                        allkeys += tag+' '\n",
    "                        temp[countCOL][index] = tag+':'+str(filterWords_list2.count(tag))\n",
    "                        countCOL +=1\n",
    "                    temp[countCOL][index] = allkeys\n",
    "                    index +=1\n",
    "            return temp\n",
    "                \n",
    "        except:\n",
    "            print('Input data type error!') \n",
    "            \n",
    "            \n",
    "# 新爬PTT\n",
    "crawlTest = CrawlPTT(PTT_URL)\n",
    "crawlTest.Crawl_commend(500) # 爬500篇文章及其推文\n",
    "crawlTest.saveCrawl()\n",
    "pddata_byname, pddata_byip, pdtwit_byname, pdtwit_byip = crawlTest.collectCommend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
